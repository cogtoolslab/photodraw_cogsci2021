{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook (which should eventually be converted to a script) will upload images to Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- what the pieces are and what assumptions are being made\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import boto3\n",
    "import botocore\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(path, ext='png'):\n",
    "    result = [y for x in os.walk(path) for y in glob(os.path.join(x[0], '*.%s' % ext))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper to speed things up by not uploading images if they already exist, can be overriden \n",
    "def check_exists(s3, bucket_name, stim_name):\n",
    "    try:\n",
    "        s3.Object(bucket_name,stim_name).load()    \n",
    "        return True\n",
    "    except botocore.exceptions.ClientError as e:    \n",
    "        if (e.response['Error']['Code'] == \"404\"):\n",
    "            print('The object does not exist.')\n",
    "            return False\n",
    "        else:\n",
    "            print('Something else has gone wrong with {}'.format(stim_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photodraw_testing_data = {'bucket_name': 'photodraw-testing',\n",
    "                          'path_to_stim': 'sketches',\n",
    "                          'full_stim_paths': '---------',  # use list_files(path_to_stim)\n",
    "                          'stim_name': '-----------'} # use os.path.split(path_to_file)[-1]\n",
    "\n",
    "df = pd.read_csv('photodraw32_metadata.csv')\n",
    "photodraw32_data = {'bucket_name': 'photodraw32',\n",
    "                    'path_to_stim': 'photodraw32_stims',\n",
    "                    'full_stim_paths': df.sketchy_filepath.values,\n",
    "                    's3_stim_names': df.s3_filename.values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## set up paths, etc.\n",
    "bucket_name = photodraw32_data['bucket_name'] ## which S3 bucket to upload to \n",
    "path_to_stim = photodraw32_data['path_to_stim']\n",
    "full_stim_paths = photodraw32_data['full_stim_paths']\n",
    "print('We have {} images to upload.'.format(len(full_stim_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## tell user some useful information\n",
    "print('Path to stimuli is : {}'.format(path_to_stim))\n",
    "print('Uploading to this bucket: {}'.format(bucket_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reallyRun = 0\n",
    "if reallyRun: \n",
    "\n",
    "    ## establish connection to s3 \n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    ## create a bucket with the appropriate bucket name\n",
    "    try: \n",
    "        b = s3.create_bucket(Bucket=bucket_name) \n",
    "        print('Created new bucket.')\n",
    "    except:\n",
    "        b = s3.Bucket(bucket_name)\n",
    "        print('Bucket already exists.')\n",
    "\n",
    "    ## do we want to overwrite files on s3?\n",
    "    overwrite = False\n",
    "    \n",
    "    ## set bucket and objects to public\n",
    "    b.Acl().put(ACL='public-read') ## sets bucket to public\n",
    "\n",
    "    ## now let's loop through stim paths and actually upload to s3 (woot!)\n",
    "    for i, path_to_file in enumerate(full_stim_paths):        # use sorted(full_stim_paths) when not using photodraw32\n",
    "        stim_name = photodraw32_data['s3_stim_names'][i]        # default: os.path.split(path_to_file)[-1]\n",
    "        if ((check_exists(s3, bucket_name, stim_name)==False) | (overwrite==True)):\n",
    "            print('Now uploading {} as {} | {} of {}'.format(os.path.split(path_to_file)[-1],stim_name,(i+1),len(full_stim_paths)))\n",
    "            s3.Object(bucket_name,stim_name).put(Body=open(path_to_file,'rb')) ## upload stimuli\n",
    "            s3.Object(bucket_name,stim_name).Acl().put(ACL='public-read') ## set access controls\n",
    "        else: \n",
    "            print('Skipping {} | {} of {} because it already exists.'.format(os.path.split(path_to_file)[-1],(i+1),len(full_stim_paths)))\n",
    "        clear_output(wait=True)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example aws output: https://photodraw32.s3.amazonaws.com/n02274259_18407_butterfly_19.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for my_bucket_object in b.objects.all():\n",
    "    print(my_bucket_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
