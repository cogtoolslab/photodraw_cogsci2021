{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conditional-armor",
   "metadata": {},
   "source": [
    "### Upload all data into an S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import botocore\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(path, ext = 'npy'):\n",
    "    return [y for x in os.walk(path) for y in glob(os.path.join(x[0], '*.{}'.format(ext) ))]\n",
    "\n",
    "def check_exists(s3, bucket_name, filename):\n",
    "    '''\n",
    "    helper to speed things up by not uploading images if they already exist, can be overriden \n",
    "    '''\n",
    "    try:\n",
    "        s3.Object(bucket_name,filename).load()    \n",
    "        return True\n",
    "    except botocore.exceptions.ClientError as e:    \n",
    "        if (e.response['Error']['Code'] == \"404\"):\n",
    "            print('The object does not exist.')\n",
    "            return False\n",
    "        else:\n",
    "            print('Something else has gone wrong with {}'.format(filename))\n",
    "            print('error is {}'.format(e.response['Error']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dir = os.getcwd()\n",
    "project_dir  = os.path.abspath('..')\n",
    "results_dir  = os.path.join(project_dir,'results')\n",
    "plot_dir     = os.path.join(results_dir,'plots')\n",
    "csv_dir      = os.path.join(results_dir,'csv')\n",
    "feature_dir  = os.path.join(project_dir, 'features')\n",
    "gallery_dir  = os.path.join(project_dir, 'gallery')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-alberta",
   "metadata": {},
   "source": [
    "Get the files we want in csv directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = list_files(csv_dir, ext = 'csv')\n",
    "csv_df    = pd.Series(csv_files).str.split('\\\\', expand = True).drop(columns=0)\n",
    "csv_df[1] = '..'\n",
    "csv_df    = csv_df[csv_df[4].str.contains('photodraw')].drop(15)\n",
    "csv_files = [os.path.join(*csv_df.values.tolist()[i]) for i in range(len(csv_df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-philadelphia",
   "metadata": {},
   "source": [
    "Get the files we want in feature directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_files = ['..\\\\features\\\\photodraw12\\\\photodraw_instance_features.npy',\n",
    "                 '..\\\\features\\\\photodraw12\\\\FEATURES_FC6_photodraw_sketch.npy',\n",
    "                 '..\\\\features\\\\FEATURES_FC6_photodraw2x2_image.npy',\n",
    "                 '..\\\\features\\\\FEATURES_FC6_photodraw2x2_sketch.npy',\n",
    "                 '..\\\\features\\\\photodraw2x2_instance_features.npy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-weekly",
   "metadata": {},
   "source": [
    "Get the png data we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "photodraw2x2_sketches = list_files('..\\\\sketches\\\\photodraw2x2', ext='png')\n",
    "photodraw2x2_stims    = list_files('..\\\\stimuli\\\\photodraw32_stims', ext='png')\n",
    "photodraw_sketches    = list_files('..\\\\sketches\\\\photodraw', ext='png')\n",
    "photodraw_stims       = list_files('..\\\\stimuli\\\\photodraw_stims', ext='png')\n",
    "participant_gallery   = list_files('..\\\\gallery', ext='png')\n",
    "stims_2x2_gallery     = list_files('..\\\\gallery', ext='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-incident",
   "metadata": {},
   "source": [
    "Consolidate paths into one big list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = [*csv_files, \n",
    "              *feature_files, \n",
    "              *photodraw2x2_sketches, \n",
    "              *photodraw2x2_stims, \n",
    "              *photodraw_sketches, \n",
    "              *photodraw_stims, \n",
    "              *participant_gallery, \n",
    "              *stims_2x2_gallery]\n",
    "data_paths = [path[2:] for path in data_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-incidence",
   "metadata": {},
   "source": [
    "Upload data into photodraw-public s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name='photodraw-public'\n",
    "\n",
    "## tell user some useful information\n",
    "\n",
    "print('Uploading to this bucket: {}'.format(bucket_name))\n",
    "\n",
    "## establish connection to s3 \n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "## create a bucket with the appropriate bucket name\n",
    "try: \n",
    "    b = s3.create_bucket(Bucket=bucket_name) \n",
    "    print('Created new bucket.')\n",
    "except:\n",
    "    b = s3.Bucket(bucket_name)\n",
    "    print('Bucket already exists.')\n",
    "\n",
    "## do we want to overwrite files on s3?\n",
    "overwrite = False\n",
    "\n",
    "## set bucket and objects to public\n",
    "b.Acl().put(ACL='public-read') ## sets bucket to public\n",
    "\n",
    "## now let's loop through data paths and actually upload to s3 \n",
    "for i, path_to_file in enumerate(data_paths):\n",
    "    filename = os.path.split(path_to_file)[-1]\n",
    "    dirname  = os.path.split(path_to_file)[-2]\n",
    "    keyname  = os.path.join(dirname,filename).replace('\\\\', '/')[1:]\n",
    "\n",
    "    if ((check_exists(s3, bucket_name, keyname)==False) | (overwrite==True)):\n",
    "        print('Now uploading {} | {} of {}'.format(path_to_file.split('/')[-1],(i+1),len(data_paths)))\n",
    "\n",
    "        # extra insurance that the stuff we don't want public isnt public\n",
    "        if filename.split('.')[-1] == 'csv':\n",
    "            df = pd.read_csv('..'+path_to_file)\n",
    "            for bad_colname in ['workerID', 'prolificID', 'Unnamed: 0', 'Unnamed: 1']:\n",
    "                if any(str(col) == bad_colname for col in df.columns.values):\n",
    "                    df = df.drop(columns=bad_colname)\n",
    "                    df.to_csv('..'+path_to_file, index=False)\n",
    "                    print(f'Removed {bad_colname} from {filename}.')\n",
    "\n",
    "\n",
    "        s3.Object(bucket_name,keyname).upload_file('..'+path_to_file) ## upload stimuli\n",
    "        s3.Object(bucket_name,keyname).Acl().put(ACL='public-read') ## set access controls\n",
    "    else: \n",
    "        print('Skipping {} | {} of {} because it already exists.'.format(path_to_file.split('/')[-1],(i+1),len(data_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1 for _ in b.objects.all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
