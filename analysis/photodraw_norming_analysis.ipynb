{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analyses and cleaning up data from typicality study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages and set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import heapq\n",
    "import socket\n",
    "import random\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo as pm\n",
    "from PIL import Image\n",
    "from urllib import request\n",
    "from math import floor, ceil\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import chisquare\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import utils\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "analysis_dir = os.getcwd()\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "ratings_dir = os.path.join(plot_dir,'ratings')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'experiments'))\n",
    "sketch_dir = os.path.abspath(os.path.join(proj_dir,'sketches'))\n",
    "gallery_dir = os.path.abspath(os.path.join(proj_dir,'gallery'))\n",
    "if socket.gethostname() == 'nightingale':\n",
    "    feature_dir = os.path.abspath('/mnt/pentagon/photodraw/features/')\n",
    "else:\n",
    "    feature_dir = os.path.abspath(os.path.join(proj_dir,'features'))\n",
    "stims_gallery_dir = os.path.join(gallery_dir, 'stims')\n",
    "    \n",
    "meta_path = os.path.abspath(os.path.join(feature_dir, 'metadata_pixels.csv'))\n",
    "image_path = os.path.abspath(os.path.join(feature_dir, 'flattened_sketches_pixels.npy'))\n",
    "meta_path_fc6 = os.path.abspath(os.path.join(feature_dir, 'METADATA_sketch.csv'))\n",
    "image_path_fc6 = os.path.abspath(os.path.join(feature_dir, 'FEATURES_FC6_sketch_no-channel-norm.npy'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'utils') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'utils'))   \n",
    "\n",
    "def make_dir_if_not_exists(dir_name):   \n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return dir_name\n",
    "\n",
    "## create directories that don't already exist        \n",
    "result = [make_dir_if_not_exists(x) for x in [results_dir,plot_dir,csv_dir,\n",
    "                                              sketch_dir,gallery_dir,feature_dir, \n",
    "                                              ratings_dir,stims_gallery_dir]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### establish connection to mongo\n",
    "\n",
    "`ssh -fNL 27020:127.0.0.1:27017 jyang@cogtoolslab.org`  \n",
    "`ssh -fNL 27017:127.0.0.1:27017 jyang@cogtoolslab.org`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set vars \n",
    "\n",
    "# this auth.txt file contains the password for the sketchloop user\n",
    "auth = pd.read_csv(os.path.join(analysis_dir,'auth.txt'), header = None) \n",
    "pswd = auth.values[0][0]\n",
    "decoderpswd = int(pswd[-1])\n",
    "user = 'sketchloop'\n",
    "host = 'cogtoolslab.org'\n",
    "\n",
    "# have to fix this to be able to analyze from local\n",
    "import socket\n",
    "if socket.gethostname().split('_')[0]=='Justin':\n",
    "    conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1:27020')\n",
    "else:\n",
    "    conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1:27017')\n",
    "db = conn['photodraw']\n",
    "coll = db['sketchy32']\n",
    "\n",
    "iterationName = 'livetest0'\n",
    "coll.distinct('iterationName')\n",
    "\n",
    "# get a sample: coll.find_one({'iterationName':iterationName})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic demographic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_responses = coll.find({\n",
    "            'iterationName':'livetest1', \n",
    "            'prolificID': {'$exists' : True},\n",
    "            'studyID': {'$exists' : True},\n",
    "            'sessionID': {'$exists' : True},\n",
    "            'eventType': 'survey'\n",
    "})\n",
    "K_responses = pd.DataFrame(K_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = K_responses[(K_responses.trial_type == 'survey-text') & \n",
    "            (K_responses.prolificID.isin(K.groupby('prolificID').prolificID.first().values))]\n",
    "print(a.responses.apply(lambda row: ast.literal_eval(row)['participantAge']).astype(int).values.mean())\n",
    "\n",
    "b = K_responses[(K_responses.trial_type != 'survey-text') & \n",
    "            (K_responses.prolificID.isin(K.groupby('prolificID').prolificID.first().values))]\n",
    "print(b.responses.apply(lambda row: ast.literal_eval(row)['participantSex']).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K_responses = coll.find({\n",
    "            'iterationName':'livetest1', \n",
    "            'prolificID': {'$exists' : True},\n",
    "            'studyID': {'$exists' : True},\n",
    "            'sessionID': {'$exists' : True},\n",
    "            'eventType': 'rating-task'\n",
    "})\n",
    "K_responses = pd.DataFrame(K_responses)\n",
    "\n",
    "K_flags = coll.find({\n",
    "            'iterationName': 'livetest1',\n",
    "            'prolificID': {'$exists' : True},\n",
    "            'studyID': {'$exists' : True},\n",
    "            'sessionID': {'$exists' : True},\n",
    "            'eventType': 'trial-catches'\n",
    "})\n",
    "K_flags = pd.DataFrame(K_flags)\n",
    "\n",
    "# remove those who never completed the experiment\n",
    "dropped_out = set(K_responses.prolificID.unique()).difference(set(K_flags.prolificID.unique()))\n",
    "K_responses = K_responses[~K_responses.prolificID.isin(dropped_out)]\n",
    "\n",
    "# make map from prolificID to flags and use map to extend to K_responses\n",
    "id_to_flags = dict(zip(K_flags['prolificID'].values, K_flags[['failed_catches', 'num_failed','lazy_responder','repeat_offender']].values.tolist()))\n",
    "\n",
    "df1 = K_responses['prolificID'].map(id_to_flags)\n",
    "df1 = pd.DataFrame(df1.tolist(), df1.index, columns = ['failed_catches', 'num_failed','lazy_responder','repeat_offender'])\n",
    "\n",
    "K_responses = pd.concat([K_responses, df1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat the response data to only include values from ['Not At All', ..., 'Extremely']\n",
    "value_list = [json.loads(dic)['typicality'] for dic in K_responses.responses.values]\n",
    "K_responses = K_responses.assign(ratings = value_list)\n",
    "\n",
    "# drop unneccesary columns and drop empty ratings\n",
    "K_responses = K_responses.drop(columns=['devMode', 'preamble', 'required', 'questions', \n",
    "                                        'randomize_question_order', 'scale_width', 'button_label', 'responses',\n",
    "                                        'question_order', 'trial_type', 'internal_node_id'])\n",
    "K_responses['ratings'] = K_responses['ratings'].replace('', np.nan)\n",
    "K_responses = K_responses.dropna(subset=['ratings'])\n",
    "\n",
    "di = {\"Not At All\": -2, \"Somewhat\": -1, \"Moderately\": 0, \"Very\": 1, \"Extremely\": 2}\n",
    "K_responses['enumerated_ratings'] = K_responses['ratings'].map(lambda x: di[x])\n",
    "\n",
    "# if a participant has more than 136 trials, find the gameID which has 136 entries and just use the trials from that gameID\n",
    "for pid in K_responses.prolificID.unique():\n",
    "    if K_responses[K_responses.prolificID == pid].shape[0] != 136:\n",
    "        subset = K_responses[K_responses.prolificID == pid]\n",
    "        for gid in subset.gameID.unique():\n",
    "            if subset[subset.gameID == gid].shape[0] == 136:\n",
    "                K_responses = K_responses[K_responses.prolificID != pid].append(subset[subset.gameID == gid], \n",
    "                                                                                ignore_index = True)\n",
    "\n",
    "# merge catch trial and main trial URLs\n",
    "K_responses = K_responses.sort_values('img_id')\n",
    "K_responses['img_id'] = K_responses['img_id'].fillna(K_responses['img_url'])\n",
    "K_responses = K_responses.reset_index(drop=True).drop(columns=['img_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some trials (batch 0) were encoding the 'enumerated ratings' values wrong. this code fixes it\n",
    "\n",
    "catch_trials = {'stimuli/catch_trials/0_ford_truck.jpg':[1,2], 'stimuli/catch_trials/1_german_shepherd.jpg':[1,2], \n",
    "                'stimuli/catch_trials/2_oddtruck.jpg':[-2,-1], 'stimuli/catch_trials/3_bedlington_terrier.jpg':[-2,-1], \n",
    "                'stimuli/catch_trials/4_pickup_truck.jpg':[1,2], 'stimuli/catch_trials/5_wierd_dog.jpg':[-2,-1], \n",
    "                'stimuli/catch_trials/6_wierdtruck.jpg':[-2,-1], 'stimuli/catch_trials/7_golden_retriever.jpg':[1,2]}\n",
    "for i in K_responses.prolificID.unique():\n",
    "    ppt_sub = K_responses[K_responses.prolificID == i]\n",
    "    counter = 0\n",
    "    for key, values in catch_trials.items():\n",
    "        if ppt_sub[ppt_sub.img_id == key]['enumerated_ratings'].values[0] not in values:\n",
    "            counter += 1\n",
    "    K_responses.loc[(K_responses.prolificID == i), 'num_failed'] = counter\n",
    "    if counter >= 4:\n",
    "        K_responses.loc[(K_responses.prolificID == i), 'failed_catches'] = True\n",
    "    else:\n",
    "        K_responses.loc[(K_responses.prolificID == i), 'failed_catches'] = False\n",
    "x=K_responses.groupby('prolificID')[['num_failed', 'failed_catches', 'repeat_offender']].mean()\n",
    "# drop participants who failed either catch trials or repeat_offender\n",
    "K_responses = K_responses[(K_responses.failed_catches == False) & (K_responses.repeat_offender == False)]\n",
    "K_responses.shape, int(K_responses.shape[0]/136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dictionary with key: prolificID, value: subset of K_responses corresponding to that prolificID\n",
    "b = {k: v for (k, v) in K_responses.groupby('prolificID')}\n",
    "keys = list(b.keys())\n",
    "\n",
    "# get just the enumerated ratings in the form of a 2D numpy array\n",
    "list_ = []\n",
    "for i in keys:\n",
    "    list_.append(b[i].enumerated_ratings.values)\n",
    "participant_ratings = np.stack(list_, axis=0)\n",
    "\n",
    "corr_ratings = pd.DataFrame(data = np.corrcoef(participant_ratings), columns = keys, index = keys)\n",
    "euc_ratings = pd.DataFrame(squareform(pdist(participant_ratings,metric='euclidean')),columns=keys,index=keys)\n",
    "cos_ratings = pd.DataFrame(squareform(pdist(participant_ratings,metric='cosine')),columns=keys,index=keys)\n",
    "corr_vals = corr_ratings.mean().values\n",
    "euc_values = euc_ratings.mean().values\n",
    "cos_values = cos_ratings.mean().values\n",
    "\n",
    "corr_vals_adj = np.abs(corr_vals - np.median(corr_vals))/(1/len(corr_vals)*sum(np.abs(corr_vals - np.mean(corr_vals))))\n",
    "euc_values_adj = np.abs(euc_values - np.median(euc_values))/(1/len(euc_values)*sum(np.abs(euc_values - np.mean(euc_values))))\n",
    "cos_values_adj = np.abs(cos_values - np.median(cos_values))/(1/len(cos_values)*sum(np.abs(cos_values - np.mean(cos_values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop values whose adjusted correlation z-scores are greater than 3 on any of the three distance/correlation metrics\n",
    "tooDissimilar = dict(zip(corr_ratings.mean().index.values, \n",
    "                        (corr_vals_adj >= 3) | (euc_values_adj >= 3) | (cos_values_adj >= 3)))\n",
    "K_responses['tooDissimilar'] = K_responses['prolificID'].map(tooDissimilar)\n",
    "K_responses = K_responses[K_responses.tooDissimilar == False]\n",
    "K_responses = K_responses.drop(columns = 'prolificID')\n",
    "K_responses.shape, int(K_responses.shape[0]/136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out CSV and sync to git\n",
    "K_responses.to_csv(os.path.join(csv_dir, 'photodraw_sketchy32_typicality_ratings.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_responses = pd.read_csv(os.path.join(csv_dir, 'photodraw_sketchy32_typicality_ratings.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ensure that all batches have 10 raters\n",
    "K_responses.groupby('batch_num')['gameID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at pairwise correlations between participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate a dictionary with key: gameID, value: subset of K_responses corresponding to that gameID\n",
    "b = {k: v for (k, v) in K_responses.groupby('gameID')}\n",
    "keys = list(b.keys())\n",
    "\n",
    "# get just the enumerated ratings in the form of a 2D numpy array\n",
    "list_ = []\n",
    "for i in keys:\n",
    "    list_.append(b[i].enumerated_ratings.values)\n",
    "participant_ratings = np.stack(list_, axis=0)\n",
    "\n",
    "corr_ratings = pd.DataFrame(data = np.corrcoef(participant_ratings), columns = keys, index = keys)\n",
    "euc_ratings = pd.DataFrame(squareform(pdist(participant_ratings,metric='euclidean')),columns=keys,index=keys)\n",
    "cos_ratings = pd.DataFrame(squareform(pdist(participant_ratings,metric='cosine')),columns=keys,index=keys)\n",
    "corr_vals = corr_ratings.mean().values\n",
    "euc_values = euc_ratings.mean().values\n",
    "cos_values = cos_ratings.mean().values\n",
    "\n",
    "corr_vals_adj = np.abs(corr_vals - np.median(corr_vals))/(1/len(corr_vals)*sum(np.abs(corr_vals - np.mean(corr_vals))))\n",
    "euc_values_adj = np.abs(euc_values - np.median(euc_values))/(1/len(euc_values)*sum(np.abs(euc_values - np.mean(euc_values))))\n",
    "cos_values_adj = np.abs(cos_values - np.median(cos_values))/(1/len(cos_values)*sum(np.abs(cos_values - np.mean(cos_values))))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr_ratings, square=True)\n",
    "plt.title('pairwise correlation between responses of two participants');\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(euc_ratings, square=True);\n",
    "plt.title('euclidean distance between responses of two participants');\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(cos_ratings, square=True);\n",
    "plt.title('cosine distance between responses of two participants');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at distribution of correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(corr_vals_adj)\n",
    "plt.xlabel('mean correlation coefficient, adjusted')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('distribution of adjusted mean correlations for each participant');\n",
    "plt.show()\n",
    "\n",
    "plt.hist(euc_values_adj)\n",
    "plt.xlabel('mean euclidean distance, adjusted')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('distribution of adjusted mean euclidean distances for each participant');\n",
    "plt.show()\n",
    "\n",
    "plt.hist(cos_values_adj)\n",
    "plt.xlabel('mean cosine distance, adjusted')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('distribution of adjusted mean cosine distances for each participant');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the distribution of the norming data look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,4)) \n",
    "\n",
    "sns.barplot(data=K_responses, \n",
    "            x='category', \n",
    "            y='enumerated_ratings', \n",
    "            order=K_responses.groupby('category').mean().sort_values('enumerated_ratings').index)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "          rotation_mode=\"anchor\");\n",
    "plt.xlabel('')\n",
    "plt.ylabel('enumerated rating')\n",
    "plt.title('ratings for each category');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_responses.groupby('category').mean().sort_values('enumerated_ratings').index.values[0+2:16+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_responses.groupby('category').mean().sort_values('enumerated_ratings').index.values[16+2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [\"Not At All\", \"Somewhat\", \"Moderately\", \"Very\", \"Extremely\"]\n",
    "fig, ax = plt.subplots(figsize=(6,4)) \n",
    "\n",
    "sns.countplot(data=K_responses, x='ratings', order=order)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "          rotation_mode=\"anchor\");\n",
    "plt.title('number of ratings in each bin, after preprocessing');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(K_responses, col='gameID', col_wrap=3, aspect=2)\n",
    "g.map(sns.countplot, \"ratings\", order=order, palette='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(K_responses, col='category', col_wrap=5, aspect=2)\n",
    "g.map(sns.countplot, \"ratings\", order=order, palette='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show countplots for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot and save count plots for each category\n",
    "# note: does not divide the graph into 3x3 segments!\n",
    "\n",
    "for cat in K_responses.category.unique():\n",
    "    instdf = K_responses[K_responses.category == cat][['img_id', 'enumerated_ratings']].sort_values('enumerated_ratings')\n",
    "\n",
    "    gb_df = instdf.groupby([\"img_id\", \"enumerated_ratings\"]).size().reset_index(name = \"counts\")\n",
    "    gb_df.sort_values([\"img_id\", \"enumerated_ratings\", \"counts\"], ascending = True, inplace = True)\n",
    "    order = K_responses[K_responses.category == cat\n",
    "                       ].groupby('img_id')['enumerated_ratings'].mean().sort_values(ascending=False).index.values\n",
    "    colors = {i:np.random.random(3,) for i in order}\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize = (12, 6))\n",
    "    ax = fig.add_subplot()\n",
    "\n",
    "    for x in order:\n",
    "\n",
    "        # get x and y values for each group\n",
    "        x_values = gb_df[gb_df[\"img_id\"] == x][\"img_id\"]\n",
    "        y_values = gb_df[gb_df[\"img_id\"] == x][\"enumerated_ratings\"]\n",
    "\n",
    "        # extract the size of each group to plot\n",
    "        size = gb_df[gb_df[\"img_id\"] == x][\"counts\"]\n",
    "\n",
    "        # extract the color for each group and covert it from rgb to hex\n",
    "        color = matplotlib.colors.rgb2hex(colors[x])\n",
    "\n",
    "        # plot the data\n",
    "        ax.scatter(x_values, y_values, s = size*50, alpha=1, cmap = colors)\n",
    "    ax.set_title(\"Count plot: \" + cat);\n",
    "    ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    sns.pointplot(x='img_id', y='enumerated_ratings', data=instdf,\n",
    "                  ci=None, order=order, scale = .7, color='grey').set(xticklabels=[], \n",
    "                                                                      xlabel='unique image', ylabel='enumerated rating');\n",
    "    ax.get_yaxis().set_major_locator(pylab.MaxNLocator(integer=True))\n",
    "    plt.savefig(os.path.join(ratings_dir, cat + '_ratings.pdf'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# split data into low/med/high typicality. In this case, it was split evenly within each category\n",
    "\n",
    "df = pd.DataFrame(K_responses[K_responses.catch_trial == False].\n",
    "                  groupby(['img_id', 'category'])['enumerated_ratings'].mean().sort_values())\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "low_typicality, med_typicality, high_typicality = [], [], []\n",
    "for name, group in df.groupby('category'):\n",
    "    sorted_vals = group.sort_values('enumerated_ratings')['img_id'].values\n",
    "    low, med, high = np.array_split(sorted_vals, 3) # note: this is not an even split -- 11/11/10\n",
    "    low_typicality.extend(low)\n",
    "    med_typicality.extend(med)\n",
    "    high_typicality.extend(high)\n",
    "    \n",
    "df.loc[(df.img_id.isin(low_typicality)), 'typicality'] = 'low'\n",
    "df.loc[(df.img_id.isin(med_typicality)), 'typicality'] = 'medium'\n",
    "df.loc[(df.img_id.isin(high_typicality)), 'typicality'] = 'high'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_responses.loc[(K_responses.img_id.isin(low_typicality)), 'typicality'] = 'low'\n",
    "K_responses.loc[(K_responses.img_id.isin(med_typicality)), 'typicality'] = 'medium'\n",
    "K_responses.loc[(K_responses.img_id.isin(high_typicality)), 'typicality'] = 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we also want to account for consistency?\n",
    "\n",
    "df2 = pd.DataFrame(K_responses[K_responses.catch_trial == False].\n",
    "                  groupby(['img_id', 'category'])['enumerated_ratings']\n",
    "                  .agg(['mean','var']))#.sort_values(by='enumerated_ratings'))\n",
    "df2.reset_index(inplace=True)\n",
    "\n",
    "x1,x2 = df2[['mean']].values, df2[['var']].values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x1_scaled, x2_scaled = min_max_scaler.fit_transform(x1), min_max_scaler.fit_transform(x2)\n",
    "df2['mean_norm'] = x1_scaled\n",
    "df2['var_norm'] = x2_scaled\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> \n",
    "### Generate galleries by typicality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reallyrun = False\n",
    "\n",
    "if reallyrun: \n",
    "    for category in K_responses[K_responses.catch_trial == False].category.unique():\n",
    "        frame = K_responses[K_responses.category == category]\n",
    "        frame = frame.groupby('img_id', as_index=False\n",
    "                             )['enumerated_ratings'].mean().sort_values('enumerated_ratings', ascending=False)\n",
    "        fig = plt.figure(figsize=(16,8),frameon=False)\n",
    "        for i,path in enumerate(frame.img_id.values):\n",
    "            request.urlretrieve(path,'temp.png')\n",
    "            img = Image.open(\"temp.png\")\n",
    "            p = plt.subplot(4,8,i+1)\n",
    "            plt.imshow(img)\n",
    "            k = p.get_xaxis().set_ticklabels([])\n",
    "            k = p.get_yaxis().set_ticklabels([])\n",
    "            k = p.get_xaxis().set_ticks([])\n",
    "            k = p.get_yaxis().set_ticks([])\n",
    "            p.axis('off')\n",
    "        plt.savefig(os.path.join(stims_gallery_dir, category + '_sorted.pdf'))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of above\n",
    "\n",
    "fig = plt.figure(figsize=(16,8),frameon=False)\n",
    "for i,path in enumerate(K_responses[K_responses.category == 'cat'].img_id.unique()):\n",
    "    request.urlretrieve(path,'temp.png')\n",
    "    img = Image.open(\"temp.png\")\n",
    "    p = plt.subplot(4,8,i+1)\n",
    "    plt.imshow(img)\n",
    "    k = p.get_xaxis().set_ticklabels([])\n",
    "    k = p.get_yaxis().set_ticklabels([])\n",
    "    k = p.get_xaxis().set_ticks([])\n",
    "    k = p.get_yaxis().set_ticks([])\n",
    "    p.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make file of just the sketchy dataset ids used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photodraw32_meta_path = os.path.join(proj_dir,'stimuli', 'photodraw32_metadata.csv')\n",
    "sketchy_filenames_outpath = os.path.join(proj_dir,'stimuli', 'photodraw32_sketchy_photoids.npy')\n",
    "\n",
    "image_metadata = pd.read_csv(photodraw32_meta_path)\n",
    "np.save(sketchy_filenames_outpath, image_metadata['sketchy_filename'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### How many people are picking the same rating per image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_responses_batch0 = K_responses[K_responses.batch_num == 0]\n",
    "arr = []\n",
    "arr2 = []\n",
    "for img in K_responses_batch0.img_id.unique():    # there is a random nan value at index 17\n",
    "    tmpa = K_responses_batch0[(K_responses_batch0.img_id == img) & (K_responses_batch0.failed_catches == True)]\n",
    "    tmpb = K_responses_batch0[(K_responses_batch0.img_id == img) & (K_responses_batch0.failed_catches == False)]\n",
    "    tmpc = mode(K_responses_batch0[K_responses_batch0.img_id == img]['enumerated_ratings'])[0][0]\n",
    "    a = mode(tmpa['enumerated_ratings'])[1][0]/5   # 5 people failed, 5 people did not\n",
    "    b = mode(tmpb['enumerated_ratings'])[1][0]/5\n",
    "    a2 = tmpa[tmpa.enumerated_ratings == tmpc]['enumerated_ratings'].count()/5\n",
    "    b2 = tmpb[tmpb.enumerated_ratings == tmpc]['enumerated_ratings'].count()/5\n",
    "    arr.append([a,b])\n",
    "    arr2.append([a2,b2])\n",
    "arr = np.asarray(arr)\n",
    "arr2 = np.asarray(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what proportion of participants picked the modal response for each image?\n",
    "sns.distplot(arr[:,0], hist=False, rug=True, label='failed')\n",
    "sns.distplot(arr[:,1], hist=False, rug=True, label='did not fail');\n",
    "plt.xlabel('proportion of participants who shared the same response in a given image');\n",
    "plt.title('mode calculated separately for for each image');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(arr2[:,0], hist=False, rug=True, label='failed')\n",
    "sns.distplot(arr2[:,1], hist=False, rug=True, label='did not fail');\n",
    "plt.xlabel('proportion of participants who shared the same response in a given image');\n",
    "plt.title('combined mode between both types for each image');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(K_responses_batch0[K_responses_batch0.failed_catches == True]['enumerated_ratings'], label='failed');\n",
    "sns.distplot(K_responses_batch0[K_responses_batch0.failed_catches == False]['enumerated_ratings'], label='did not fail')\n",
    "plt.legend();\n",
    "plt.title('distribution of ratings between those who failed catch trials and those who did not');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How similar are different participants' responses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed form: \n",
    "# sims = [sum([ratings[i,k] == ratings[j,k] for k in range(ratings.shape[1])]) for i,j in combinations(range(10), 2)]\n",
    "\n",
    "def get_pairwise_similarity_matrix(ratings, distance = 0):\n",
    "    '''\n",
    "        ratings: a # participants x # images 2D numpy array\n",
    "        returns an upper triangular matrix that compares rating responses for participant i and participant j \n",
    "    '''\n",
    "    sims = []\n",
    "    # for each pair of participants,\n",
    "    for i,j in combinations(range(ratings.shape[0]), 2):\n",
    "        tmp = 0\n",
    "        # for each image,\n",
    "        for k in range(ratings.shape[1]):\n",
    "            # is participant j's rating 'distance' away from participant i's rating? \n",
    "            if distance != 0:\n",
    "                tmp += ratings[j,k] in range(ratings[i,k] - distance, ratings[i,k] + distance + 1) # +1 for inclusive upper bound\n",
    "            else:\n",
    "                tmp += ratings[j,k] == ratings[i,k]\n",
    "        sims.append(tmp)\n",
    "    \n",
    "    # turn it into upper triangular matrix\n",
    "    tri = np.zeros((ratings.shape[0], ratings.shape[0]))\n",
    "    tri[np.triu_indices(ratings.shape[0], 1)] = sims\n",
    "    \n",
    "    return tri/ratings.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(K_responses_batch0[K_responses_batch0.catch_trial == True][['img_id',\n",
    "                                                                  'enumerated_ratings']].sort_values('img_id').values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what specific images are the people who pass the catch trials missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dictionary with key: gameID, value: subset of K_responses_batch0 corresponding to that gameID\n",
    "a = {k: v for (k, v) in K_responses_batch0.groupby('gameID')}\n",
    "keys = list(a.keys())\n",
    "\n",
    "# get just the enumerated ratings in the form of a 2D numpy array\n",
    "participant_ratings = np.stack([a[i].enumerated_ratings.values for i in keys], axis=0)\n",
    "print('shape:', participant_ratings.shape)\n",
    "\n",
    "# get average correlation across all pairs of participants\n",
    "participant_ratings_noatypical = np.delete(participant_ratings, 2, 0)\n",
    "corrs = [np.corrcoef(participant_ratings_noatypical[i], participant_ratings_noatypical[j])[0,1] for i,j in combinations(range(9), 2)]\n",
    "a = np.asarray(corrs)\n",
    "print('average pairwise correlation across all participants, removing atypical one', a.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pass vs failed catch trials\n",
    "K_pass = K_responses_batch0[(K_responses_batch0.failed_catches == False)]\n",
    "K_fail = K_responses_batch0[(K_responses_batch0.failed_catches == True)]\n",
    "\n",
    "# generate a dictionary with key: gameID, value: subset of K_responses_batch0 corresponding to that gameID\n",
    "a = {k: v for (k, v) in K_pass.groupby('gameID')}\n",
    "keys = list(a.keys())\n",
    "# get just the enumerated ratings in the form of a 2D numpy array \n",
    "participant_ratings_pass_noatypical = np.stack([a[i].enumerated_ratings.values for i in keys], axis=0)\n",
    "\n",
    "# generate a dictionary with key: gameID, value: subset of K_responses_batch0 corresponding to that gameID\n",
    "a = {k: v for (k, v) in K_fail.groupby('gameID')}\n",
    "keys = list(a.keys())\n",
    "# get just the enumerated ratings in the form of a 2D numpy array\n",
    "participant_ratings_fail_noatypical = np.stack([a[i].enumerated_ratings.values for i in keys], axis=0)\n",
    "print(\"passing vs failed participant responses shape:\",participant_ratings_pass_noatypical.shape, participant_ratings_fail_noatypical.shape)\n",
    "\n",
    "# compute average correlation for set who passed catch trials\n",
    "corrs = [np.corrcoef(participant_ratings_pass_noatypical[i], participant_ratings_pass_noatypical[j])[0,1] for i,j in combinations(range(participant_ratings_pass_noatypical.shape[0]), 2)]\n",
    "pass_corr = np.asarray(corrs)\n",
    "print('participants who passed catch trials, for all trials:', pass_corr.mean())\n",
    "\n",
    "# compute average correlation for set who failed catch trials\n",
    "corrs = [np.corrcoef(participant_ratings_fail_noatypical[i], participant_ratings_fail_noatypical[j])[0,1] for i,j in combinations(range(participant_ratings_fail_noatypical.shape[0]), 2)]\n",
    "fail_corr = np.asarray(corrs)\n",
    "print('participants who failed catch trials, for all trials:', fail_corr.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- try between pass and fail, are their response vectors systematically different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up isCatchTrial x failedCatches 2x2 \n",
    "K_catch = K_responses_batch0[(K_responses_batch0.catch_trial == True)]\n",
    "K_main = K_responses_batch0[(K_responses_batch0.catch_trial == False)]\n",
    "\n",
    "K_catch_pass = K_catch[K_catch.failed_catches == False]\n",
    "K_catch_fail = K_catch[K_catch.failed_catches == True]\n",
    "K_main_pass = K_main[K_main.failed_catches == False]\n",
    "K_main_fail = K_main[K_main.failed_catches == True]\n",
    "print('dataframe shapes:', K_catch_pass.shape, K_catch_fail.shape, K_main_pass.shape, K_main_fail.shape)\n",
    "\n",
    "# extract ratings only as 2D numpy array for 2x2 set\n",
    "a = {k: v for (k, v) in K_catch_pass.groupby('gameID')}\n",
    "keys = list(a.keys())\n",
    "participant_ratings_catch_pass = np.stack([a[i].enumerated_ratings.values for i in keys], axis=0)\n",
    "\n",
    "a = {k: v for (k, v) in K_catch_fail.groupby('gameID')}\n",
    "keys = list(a.keys())\n",
    "participant_ratings_catch_fail = np.stack([a[i].enumerated_ratings.values for i in keys], axis=0)\n",
    "\n",
    "a = {k: v for (k, v) in K_main_pass.groupby('gameID')}\n",
    "keys = list(a.keys())\n",
    "participant_ratings_main_pass = np.stack([a[i].enumerated_ratings.values for i in keys], axis=0)\n",
    "\n",
    "a = {k: v for (k, v) in K_main_fail.groupby('gameID')}\n",
    "keys = list(a.keys())\n",
    "participant_ratings_main_fail = np.stack([a[i].enumerated_ratings.values for i in keys], axis=0)\n",
    "\n",
    "print('2D numpy shapes', participant_ratings_catch_pass.shape, participant_ratings_catch_fail.shape, participant_ratings_main_pass.shape, participant_ratings_main_fail.shape)\n",
    "\n",
    "# get average correlation for for the 2x2 set\n",
    "corrs = [np.corrcoef(participant_ratings_catch_pass[i], participant_ratings_catch_pass[j])[0,1] for i,j in combinations(range(participant_ratings_catch_pass.shape[0]), 2)]\n",
    "catch_pass_corr = np.asarray(corrs)\n",
    "print('catch trials only, passing participants:', catch_pass_corr.mean())\n",
    "\n",
    "corrs = [np.corrcoef(participant_ratings_catch_fail[i], participant_ratings_catch_fail[j])[0,1] for i,j in combinations(range(participant_ratings_catch_fail.shape[0]), 2)]\n",
    "catch_fail_corr = np.asarray(corrs)\n",
    "print('catch trials only, failing participants:', catch_fail_corr.mean())\n",
    "\n",
    "corrs = [np.corrcoef(participant_ratings_main_pass[i], participant_ratings_main_pass[j])[0,1] for i,j in combinations(range(participant_ratings_main_pass.shape[0]), 2)]\n",
    "main_pass_corr = np.asarray(corrs)\n",
    "print('no catch trials, passing participants:', main_pass_corr.mean())\n",
    "\n",
    "corrs = [np.corrcoef(participant_ratings_main_fail[i], participant_ratings_main_fail[j])[0,1] for i,j in combinations(range(participant_ratings_main_fail.shape[0]), 2)]\n",
    "main_fail_corr = np.asarray(corrs)\n",
    "print('no catch trials, failing participants:', main_fail_corr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dictionary with key: gameID, value: subset of K_responses_batch0 corresponding to that gameID\n",
    "b = {k: v for (k, v) in K_responses_batch0[K_responses_batch0.failed_catches == True].groupby('gameID')}\n",
    "keys = list(b.keys())\n",
    "\n",
    "# get just the enumerated ratings in the form of a 2D numpy array\n",
    "list_ = []\n",
    "for i in keys:\n",
    "    list_.append(b[i].enumerated_ratings.values)\n",
    "participant_ratings_failed = np.stack(list_, axis=0)\n",
    "\n",
    "# generate a dictionary with key: gameID, value: subset of K_responses_batch0 corresponding to that gameID\n",
    "c = {k: v for (k, v) in K_responses_batch0[K_responses_batch0.failed_catches == False].groupby('gameID')}\n",
    "keys = list(c.keys())\n",
    "\n",
    "# get just the enumerated ratings in the form of a 2D numpy array\n",
    "list_ = []\n",
    "for i in keys:\n",
    "    list_.append(c[i].enumerated_ratings.values)\n",
    "participant_ratings_passed = np.stack(list_, axis=0)\n",
    "participant_ratings_failed.shape, participant_ratings_passed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# simulate the expected standard deviation of 10 people guessing at random\n",
    "print('chance std. dev (passed):', statistics.mean([statistics.pstdev(random.choices([-2,-1,0,1,2], k=4)) for i in range(10000)]))\n",
    "# get the mean standard deviation of ratings within each image, where the range of standard deviations possible is [0,2]\n",
    "print('actual std. dev (passed):', statistics.mean([participant_ratings_passed[:, i].std() for i in range(136)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the expected standard deviation of 10 people guessing at random\n",
    "print('chance std. dev (failed):', statistics.mean([statistics.pstdev(random.choices([-2,-1,0,1,2], k=6)) for i in range(10000)]))\n",
    "# get the mean standard deviation of ratings within each image, where the range of standard deviations possible is [0,2]\n",
    "print('actual std. dev (failed):', statistics.mean([participant_ratings_failed[:, i].std() for i in range(136)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# simulate the expected standard deviation of 10 people guessing at random\n",
    "print('chance std. dev:', statistics.mean([statistics.pstdev(random.choices([-2,-1,0,1,2], k=10)) for i in range(10000)]))\n",
    "\n",
    "# get the mean standard deviation of ratings within each image, where the range of standard deviations possible is [0,2]\n",
    "print('actual std. dev:', statistics.mean([participant_ratings[:, i].std() for i in range(136)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of identical responses for all combinations of participants\n",
    "sims = get_pairwise_similarity_matrix(participant_ratings_passed, 0)\n",
    "\n",
    "# plot results\n",
    "sns.heatmap(sims, vmax = 0.5)\n",
    "plt.xlabel('participant index')\n",
    "plt.ylabel('participant index')\n",
    "plt.title('proportion of identical responses between two participants (passed)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of identical responses for all combinations of participants\n",
    "sims = get_pairwise_similarity_matrix(participant_ratings_failed, 0)\n",
    "\n",
    "# plot results\n",
    "sns.heatmap(sims, vmax = 0.5)\n",
    "plt.xlabel('participant index')\n",
    "plt.ylabel('participant index')\n",
    "plt.title('proportion of identical responses between two participants (failed)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the number of identical responses for all combinations of participants\n",
    "sims = get_pairwise_similarity_matrix(participant_ratings, 0)\n",
    "\n",
    "# plot results\n",
    "sns.heatmap(sims)\n",
    "plt.xlabel('participant index')\n",
    "plt.ylabel('participant index')\n",
    "plt.title('proportion of identical responses between two participants');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of identical responses for all combinations of participants\n",
    "sims = get_pairwise_similarity_matrix(participant_ratings, 1)\n",
    "\n",
    "# plot results\n",
    "sns.heatmap(sims)\n",
    "plt.xlabel('participant index')\n",
    "plt.ylabel('participant index')\n",
    "plt.title('proportion of responses off by 1 between two participants');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of identical responses for all combinations of participants\n",
    "sims = get_pairwise_similarity_matrix(participant_ratings, 2)\n",
    "\n",
    "# plot results\n",
    "sns.heatmap(sims)\n",
    "plt.xlabel('participant index')\n",
    "plt.ylabel('participant index')\n",
    "plt.title('proportion of responses off by 2 between two participants');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of identical responses for all combinations of participants\n",
    "sims = get_pairwise_similarity_matrix(participant_ratings, 3)\n",
    "\n",
    "# plot results\n",
    "sns.heatmap(sims)\n",
    "plt.xlabel('participant index')\n",
    "plt.ylabel('participant index')\n",
    "plt.title('proportion of responses off by 3 between two participants');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of identical responses for all combinations of participants\n",
    "sims = get_pairwise_similarity_matrix(participant_ratings, 4)\n",
    "\n",
    "# plot results\n",
    "sns.heatmap(sims)\n",
    "plt.xlabel('participant index')\n",
    "plt.ylabel('participant index')\n",
    "plt.title('proportion of responses off by 4 between two participants');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kappa statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "from nltk import agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_codes = [[index, index2, rating] for index, ppt in enumerate(participant_ratings.tolist()) for index2, rating in enumerate(ppt)]\n",
    "formatted_codes_passed = [[index, index2, rating] for index, ppt in enumerate(participant_ratings_passed.tolist()) for index2, rating in enumerate(ppt)]\n",
    "formatted_codes_failed = [[index, index2, rating] for index, ppt in enumerate(participant_ratings_failed.tolist()) for index2, rating in enumerate(ppt)]\n",
    "\n",
    "\n",
    "len(formatted_codes), len(formatted_codes_passed), len(formatted_codes_failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some pretty low scores! <br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look for a rejection criteria for counteracting people who guess/select one thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = {}\n",
    "for mini in K_responses.batch_num.unique():\n",
    "    miniset = K_responses[K_responses.batch_num == mini]\n",
    "    for ppt in miniset.gameID.unique():\n",
    "        pop = miniset[miniset.gameID != ppt].ratings.values\n",
    "        ind = miniset[miniset.gameID == ppt].ratings.values\n",
    "\n",
    "        frequency_pop, frequency_ind = {}, {}\n",
    "        for item in np.unique(pop).tolist():\n",
    "            frequency_pop[item] = pop.tolist().count(item)\n",
    "            frequency_ind[item] = ind.tolist().count(item)\n",
    "        ind_count = list(frequency_ind.values())\n",
    "        pop_count = list(frequency_pop.values())\n",
    "        pop_prop = [i/sum(pop_count) for i in pop_count]\n",
    "\n",
    "        expected_adj = [i*sum(ind_count) for i in pop_prop]\n",
    "\n",
    "        res[ppt] = chisquare(f_obs=ind_count, f_exp=expected_adj)[0:2]\n",
    "[i[1] for i in list(res.values())]\n",
    "sum([i[1]<0.01 for i in list(res.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for ppt in K_responses.gameID.unique():\n",
    "    pop = K_responses[K_responses.gameID != ppt].ratings.values\n",
    "    ind = K_responses[K_responses.gameID == ppt].ratings.values\n",
    "    \n",
    "    frequency_pop, frequency_ind = {}, {}\n",
    "    for item in np.unique(pop).tolist():\n",
    "        frequency_pop[item] = pop.tolist().count(item)\n",
    "        frequency_ind[item] = ind.tolist().count(item)\n",
    "    ind_count = list(frequency_ind.values())\n",
    "    pop_count = list(frequency_pop.values())\n",
    "    pop_prop = [i/sum(pop_count) for i in pop_count]\n",
    "    \n",
    "    expected_adj = [i*sum(ind_count) for i in pop_prop]\n",
    "    \n",
    "    res[ppt] = chisquare(f_obs=ind_count, f_exp=expected_adj)[0:2]\n",
    "sum([i[1]<0.01 for i in list(res.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = defaultdict(list)\n",
    "[myList[key] for key in K_responses.gameID.unique()]\n",
    "myList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_vals = dict([(i,[]) for i in K_responses.gameID.unique().tolist()])\n",
    "\n",
    "for img in K_responses.img_id.unique():\n",
    "    imgset = K_responses[K_responses.img_id == img]\n",
    "    for ppt in imgset.gameID.unique():\n",
    "        ind_rating = imgset[imgset.gameID == ppt].enumerated_ratings.values[0]\n",
    "\n",
    "        # for this specific image, is this participant's response near the mean response?\n",
    "        mean_ratings = imgset.enumerated_ratings.mean()\n",
    "        ppt_vals[ppt].append(floor(mean_ratings) <= ind_rating \n",
    "                             <= ceil(mean_ratings))\n",
    "\n",
    "sns.distplot([sum(ppt_vals[i]) for i in ppt_vals.keys()]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_vals = dict([(i,[]) for i in K_responses.gameID.unique().tolist()])\n",
    "\n",
    "for img in K_responses.img_id.unique():\n",
    "    imgset = K_responses[K_responses.img_id == img]\n",
    "    for ppt in imgset.gameID.unique():\n",
    "        ind_rating = imgset[imgset.gameID == ppt].enumerated_ratings.values[0]\n",
    "        \n",
    "        # for this specific image, is this participant's response the modal response?\n",
    "        ppt_vals[ppt].append(mode(imgset.enumerated_ratings.values)[0][0] == ind_rating)\n",
    "\n",
    "sns.distplot([sum(ppt_vals[i]) for i in ppt_vals.keys()])\n",
    "[sum(ppt_vals[i]) for i in ppt_vals.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.histogram([sum(ppt_vals[i]) for i in ppt_vals.keys()])\n",
    "plt.hist([sum(ppt_vals[i]) for i in ppt_vals.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_vals = dict([(i,[]) for i in K_responses.gameID.unique().tolist()])\n",
    "\n",
    "for img in K_responses.img_id.unique():\n",
    "    imgset = K_responses[K_responses.img_id == img]\n",
    "    for ppt in imgset.gameID.unique():\n",
    "        ind_rating = imgset[imgset.gameID == ppt].enumerated_ratings.values[0]\n",
    "        \n",
    "        # for this specific image, is this participant's response the modal response? Scaled by agreement\n",
    "        [ppt_vals[ppt].append(mode(imgset.enumerated_ratings.values)[0][0] == ind_rating) for i in range(mode(imgset.enumerated_ratings.values)[1][0])]\n",
    "\n",
    "sns.distplot([sum(ppt_vals[i]) for i in ppt_vals.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([sum(ppt_vals[i]) for i in ppt_vals.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = np.asarray([[i, sum(ppt_vals[i])] for i in ppt_vals.keys()])\n",
    "th = th[th[:,0].argsort()][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dictionary with key: gameID, value: subset of K_responses corresponding to that gameID\n",
    "b = {k: v for (k, v) in K_responses.groupby('gameID')}\n",
    "keys = list(b.keys())\n",
    "\n",
    "# get just the enumerated ratings in the form of a 2D numpy array\n",
    "list_ = []\n",
    "for i in keys:\n",
    "    list_.append(b[i].enumerated_ratings.values)\n",
    "participant_ratings = np.stack(list_, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of identical responses for all combinations of participants\n",
    "sims = get_pairwise_similarity_matrix(participant_ratings, 0)\n",
    "mask = np.zeros_like(sims)\n",
    "mask[np.tril_indices_from(mask)] = True\n",
    "\n",
    "\n",
    "# plot results\n",
    "fig, ax = plt.subplots(figsize=(12,10)) \n",
    "sns.heatmap(sims, square=True, ax=ax,mask=mask,  cbar_kws={\"shrink\": .5}, rasterized=False)\n",
    "ax.set_xticklabels(th, rotation=90)\n",
    "ax.set_yticklabels(th)\n",
    "\n",
    "# ax2 = fig.add_subplot(111, label=\"secondary\")\n",
    "# ax2.set_aspect(\"equal\")\n",
    "# ax2.set_xlim(ax.get_xlim())\n",
    "# ax2.set_ylim(ax.get_ylim())\n",
    "# ax2.tick_params(left=False, labelleft=False, bottom=False, labelbottom=False,\n",
    "#                 top=True, labeltop=True)\n",
    "# ax2.set_facecolor(\"none\")\n",
    "# for _, spine in ax2.spines.items():\n",
    "#     spine.set_visible(False)\n",
    "# ax2.set_xticks(ax.get_xticks())\n",
    "# ax2.set_xticklabels(keys, fontsize=13)\n",
    "# plt.setp(ax2.get_xticklabels(), rotation=45, ha=\"left\",\n",
    "#          rotation_mode=\"anchor\")\n",
    "\n",
    "\n",
    "plt.xlabel('# of modal responses for this participant')\n",
    "plt.ylabel('# of modal responses for this participant')\n",
    "plt.title('number of identical responses between two participants');\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "smh = np.asarray([np.append(sims[i, :], sims[:,i])[np.append(sims[i, :], sims[:,i]) != 0] for i in range(len(sims))])\n",
    "x,y = smh.mean(axis=1), th.astype(np.int)/136\n",
    "plt.plot(x, y, '.')\n",
    "m, b = np.polyfit(x, y, 1)\n",
    "plt.plot(x, m*x + b)\n",
    "plt.xlabel('average proportion of identical responses')\n",
    "plt.ylabel('proportion of modal ratings');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(euc_ratings.mean().values)\n",
    "plt.xlabel('mean euclidean distance')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('distribution of euclidean distances for each participant');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cos_ratings.mean().values)\n",
    "plt.xlabel('mean cosine distance')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('distribution of cosine distances for each participant');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(corr_ratings.mean().values)\n",
    "plt.xlabel('mean correlation coefficient')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('distribution of mean correlations for each participant');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(euc_ratings.mean().values)\n",
    "plt.xlabel('mean euclidean distance')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('distribution of euclidean distances for each participant');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cos_ratings.mean().values)\n",
    "plt.xlabel('mean cosine distance')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('distribution of cosine distances for each participant');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(corr_ratings.mean().values)\n",
    "plt.xlabel('mean correlation coefficient')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('distribution of mean correlations for each participant');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation of our trial according to current response frequencies \n",
    "response_freqs = np.unique(K_responses.enumerated_ratings.values, return_counts=True)\n",
    "response_freqs = dict(zip(response_freqs[0],response_freqs[1]/response_freqs[1].sum()))\n",
    "\n",
    "def generate_consecutives():\n",
    "    randresps = np.random.choice(list(response_freqs.keys()), size=136, p=list(response_freqs.values()))\n",
    "    return heapq.nlargest(2, (len(list(y)) for (c,y) in groupby(randresps)))\n",
    "\n",
    "arr = np.asarray([generate_consecutives() for i in range(10000)])\n",
    "sum((arr[:,0] >= 20) | ((arr[:,0] >= 10) & (arr[:,1] >= 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((arr[:,0] >= 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_flags.lazy_responder.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_L = [(k, sum(1 for i in g)) for k,g in groupby(randresps)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
